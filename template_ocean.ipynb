{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NByplwOPf8qJ"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9daGLI1f8qO"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajnwhnu4f8qP"
   },
   "source": [
    "## For google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu6oEd6Glqou"
   },
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "# from google.colab import files\n",
    "\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OidoI15ilqou"
   },
   "outputs": [],
   "source": [
    "# !mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets list\n",
    "# ! kaggle competitions download -c ocean-eddy-detection\n",
    "# ! mkdir train\n",
    "# ! unzip ocean-eddy-detection.zip -d train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFKvvrzOf8qR",
    "outputId": "25d8d428-deb1-45d0-ff23-e58ad03cf484"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# path = 'drive/MyDrive/SUP/4A/ocean-eddy-detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COhGiis5lqov"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVziv-ZYlqow"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "image_transforms_flips = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(1.0),\n",
    "        transforms.RandomVerticalFlip(1.0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "image_transforms_blur = transforms.Compose(\n",
    "    [\n",
    "        transforms.GaussianBlur(7, sigma=(0.1, 3.0)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbkfHRRWf8qR"
   },
   "outputs": [],
   "source": [
    "class TrainDataset():\n",
    "    def __init__(self, path):\n",
    "\n",
    "        eddies_train = xr.open_dataset(path + 'eddies_train.nc')\n",
    "        X_train = xr.open_dataset(path + 'OSSE_U_V_SLA_SST_train.nc')\n",
    "        \n",
    "        y = eddies_train.eddies.values\n",
    "        \n",
    "        X_verti = X_train.vomecrtyT.values\n",
    "        X_hori = X_train.vozocrtxT.values\n",
    "        X_SSH = X_train.sossheig.values\n",
    "        X_SST = X_train.votemper.values\n",
    "        \n",
    "        ##Transformation\n",
    "        X = np.array([X_verti, X_hori, X_SSH, X_SST])\n",
    "        X = X.transpose((1,0,2,3))\n",
    "        \n",
    "        y = np.nan_to_num(y, nan=3)\n",
    "            \n",
    "        #Enregistre les index correspondant aux bords\n",
    "        edges_index = []\n",
    "        for i in range(1,X.shape[2]-1):\n",
    "            for j in range (1,X.shape[3]-1):\n",
    "                if np.isnan(X[32, 3, i, j]):\n",
    "                    if np.any(np.isnan(X[32, 3, i-1:i+2, j-1:j+2])!=True):\n",
    "                        edges_index.append((i, j))  \n",
    "        \n",
    "        \n",
    "        for img_index in tqdm(range(X.shape[0])):\n",
    "            for index in edges_index:\n",
    "                i, j = index\n",
    "                X[img_index, :, i, j] = np.mean(X[img_index, :, i-1:i+2, j-1:j+2], axis=(1,2))\n",
    "\n",
    "        \n",
    "        X = np.nan_to_num(X, nan=0)\n",
    "    \n",
    "        ##Normalisation\n",
    "        \n",
    "        X = (X - np.min(X, axis=(0,2,3), keepdims=True))/( np.max(X, axis=(0,2,3), keepdims=True) - np.min(X, axis=(0,2,3), keepdims=True) )\n",
    "        \n",
    "        ##Augmentation \n",
    "        \n",
    "        X = torch.tensor(X)\n",
    "        y = torch.tensor(y, dtype=torch.long).reshape(y.shape[0],1, y.shape[1],y.shape[2])\n",
    "        data = torch.cat((X,y),dim=1)\n",
    "        \n",
    "        data_aug = image_transforms_flips(data)\n",
    "        \n",
    "        X_aug  = data_aug[:,:-1,:,:]\n",
    "        \n",
    "        X_aug = image_transforms_blur(X_aug)\n",
    "        \n",
    "        y_aug = data_aug[:,-1,:,:]\n",
    "        \n",
    "        self.X_train = torch.cat((X, X_aug), dim=0)\n",
    "        self.y_train = torch.cat((y.reshape(y.shape[0], y.shape[2], y.shape[3]), y_aug), dim=0)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENL68vyNlqoy"
   },
   "outputs": [],
   "source": [
    "class TestDataSet():\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.X_test = xr.open_dataset(path + '/OSSE_U_V_SLA_SST_test.nc')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_test[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WslTBN8lqoy"
   },
   "source": [
    "We split the set between one train set and one validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N0m__KWlqoz",
    "outputId": "b69cec03-6760-4361-b3ee-78f4990805ce"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "path = 'ocean-eddy-detection/' # local\n",
    "#path = 'train/' # if running on colab\n",
    "dataset = TrainDataset(path)\n",
    "\n",
    "trainDataSet, valDataSet = random_split(dataset, [0.8, 0.2])\n",
    "trainDataLoader = DataLoader(trainDataSet, batch_size=4, shuffle=True)\n",
    "valDataLoader = DataLoader(valDataSet, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"The length of train data is:\",len(trainDataSet))\n",
    "print(\"The length of validation data is:\",len(valDataSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZq-i0E8lqoz"
   },
   "outputs": [],
   "source": [
    "iterator_data = iter(trainDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "BvVBjT1slqoz",
    "outputId": "67bff4b1-a067-43ea-9c0b-32294bd887da"
   },
   "outputs": [],
   "source": [
    "img, label = next(iterator_data)\n",
    "print(img.shape)\n",
    "plt.imshow(img[0][0], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Vertical velocity')\n",
    "plt.show()\n",
    "plt.imshow(label[0], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tbwYaCxlqo0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QCVIo3820FQ"
   },
   "outputs": [],
   "source": [
    "class Unet_3_blocks(nn.Module):\n",
    "    \"\"\"\n",
    "    Our modified Unet :\n",
    "    Use of padding to keep size of input in output easily.\n",
    "    Use of batchnorm2d after Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downblock1 = nn.Sequential(\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(4, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downblock2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downblock3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.middleU = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.downblock1(x)\n",
    "\n",
    "        x2 = self.downblock2(x1)\n",
    "\n",
    "        x3 = self.downblock3(x2)\n",
    "\n",
    "        xmiddle = self.middleU(x3)\n",
    "\n",
    "        xup0_1 = torch.cat((x3,xmiddle), dim=1)\n",
    "        xup1 = self.upblock1(xup0_1)\n",
    "\n",
    "        xup1_2 = torch.cat((x2,xup1), dim=1)\n",
    "        xup2 = self.upblock2(xup1_2)\n",
    "\n",
    "        xup2_3 = torch.cat((x1,xup2), dim=1)\n",
    "        xup3 = self.upblock3(xup2_3)\n",
    "\n",
    "        return xup3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I4yDnsglqo0"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Adapted_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss(weight=torch.Tensor([1/13, 6/13, 6/13]))\n",
    "      \n",
    "    def forward(self, prediction, target):\n",
    "        mask = target!=3\n",
    "        y = target[mask]\n",
    "        ypred = prediction.transpose(0,1)\n",
    "        ypred = ypred[:, mask].transpose(0,1)\n",
    "        return self.loss(ypred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDXqrzMflqo1"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UKMFiIKf8qW"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move input and target tensors to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #pred = pred.to(dtype=torch.float64)\n",
    "    \n",
    "        loss = loss_fn(pred, y.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss and accuracy\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4_bRBWqlqo1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_one_vs_all(prediction, target):\n",
    "    pred = prediction.argmax(1).ravel()\n",
    "    ytrue = target.ravel()\n",
    "\n",
    "    F1=[]\n",
    "\n",
    "    for k in range(3):\n",
    "        class_k_true = ytrue==k\n",
    "        class_k_pred = (pred==k)*(ytrue!=3)\n",
    "        F1.append(f1_score(class_k_true,class_k_pred))\n",
    "\n",
    "    return F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPfhDZ2Dlqo1"
   },
   "outputs": [],
   "source": [
    "def val_loop(dataloader, model, loss_fn, scheduler, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    a,b,c = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).to(device)\n",
    "            test_loss += loss_fn(pred, y.long()).item()\n",
    "            f = f1_one_vs_all(pred.cpu(), y.cpu())\n",
    "            a += f[0]\n",
    "            b += f[1]\n",
    "            c += f[2]\n",
    "    test_loss /= num_batches\n",
    "    a/=size\n",
    "    b/=size\n",
    "    c/=size\n",
    "    print(f\"Test Error: \\n F1: {(100*a):>0.2f}%, {(100*b):>0.2f}%, {(100*c):>0.2f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRI2JdpX20FU"
   },
   "source": [
    "# Training for 3 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKFEI66a20FU",
    "outputId": "be3f3c04-21f6-4b2d-af7a-c7d54689a669"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_3_blocks = Unet_3_blocks()\n",
    "model_3_blocks = model_3_blocks.to(device)\n",
    "loss_fn = Adapted_loss().to(device)\n",
    "optimizer = torch.optim.Adam(model_3_blocks.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PONx8z5d20FU",
    "outputId": "980571f8-d8e4-424a-e33e-de9af692d550"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "min_loss = np.inf\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(trainDataLoader, model_3_blocks, loss_fn, optimizer, device)\n",
    "    test_loss = val_loop(valDataLoader, model_3_blocks, loss_fn, scheduler, device)\n",
    "    if min_loss > test_loss:\n",
    "        min_loss = test_loss\n",
    "        torch.save(model_3_blocks, 'UNet_3_blocks_V2_best.pth')\n",
    "print(\"Done!\")\n",
    "\n",
    "# #Uncomment to save Model\n",
    "torch.save(model_3_blocks, 'UNet_3_blocks_V2_final_training.pth')\n",
    "\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZq_DQc520FV"
   },
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('UNet_3_blocks_V2.pth', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXPT-jgV20FW"
   },
   "outputs": [],
   "source": [
    "x_test, label_test = next(iter(valDataLoader))\n",
    "print(label_test.shape)\n",
    "x_test = x_test.to(device)[0]\n",
    "pred = np.argmax(model(x_test.reshape(1,4,357,717)).softmax(dim=1).cpu().detach().numpy(), axis=1)[0]\n",
    "mask = label_test[0]==3\n",
    "pred[mask] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUX_AxO3f8qZ"
   },
   "outputs": [],
   "source": [
    "plt.imshow(label_test[0], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.imshow(pred, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='x-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Ground Truth', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(pred, label_test[0])\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e567364bc8ddf153d80a9d3509d1b5709c952b74d6848487d8a2c235834a8128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
