{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NByplwOPf8qJ"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9daGLI1f8qO"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajnwhnu4f8qP"
   },
   "source": [
    "## For google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFKvvrzOf8qR",
    "outputId": "41186f60-0a55-418d-c8a4-69bdd89a9f06"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#path = 'drive/MyDrive/SUP/4A/SDD/hackaton_ocean/ocean-eddy-detection'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(0.99),\n",
    "        transforms.RandomVerticalFlip(0.99),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbkfHRRWf8qR"
   },
   "outputs": [],
   "source": [
    "class TrainDataset():\n",
    "    def __init__(self, path):\n",
    "\n",
    "        eddies_train = xr.open_dataset(path + '/eddies_train.nc')\n",
    "        X_train = xr.open_dataset(path + '/OSSE_U_V_SLA_SST_train.nc')\n",
    "        \n",
    "        y = eddies_train.eddies.values\n",
    "        \n",
    "        X_verti = X_train.vomecrtyT.values\n",
    "        X_hori = X_train.vozocrtxT.values\n",
    "        X_SSH = X_train.sossheig.values\n",
    "        X_SST = X_train.votemper.values\n",
    "\n",
    "        X_verti = np.nan_to_num(X_verti, nan=0)\n",
    "        X_hori = np.nan_to_num(X_hori, nan=0)\n",
    "        X_SSH = np.nan_to_num(X_SSH, nan=0)\n",
    "        X_SST = np.nan_to_num(X_SST, nan=0)\n",
    "        \n",
    "        ##Normalisation\n",
    "        X_verti = ( X_verti - np.min(X_verti) )/( np.max(X_verti) - np.min(X_verti) )\n",
    "        X_hori = ( X_hori - np.min(X_hori) )/( np.max(X_hori) - np.min(X_hori) )\n",
    "        X_SSH = ( X_SSH - np.min(X_SSH) )/( np.max(X_SSH) - np.min(X_SSH) )\n",
    "        X_SST = ( X_SST - np.min(X_SST) )/( np.max(X_SST) - np.min(X_SST) )\n",
    "        \n",
    "        \n",
    "        ##Transformation\n",
    "        X = np.array([X_verti, X_hori, X_SSH, X_SST])\n",
    "        X = X.transpose((1,0,2,3))\n",
    "        y = np.nan_to_num(y, nan=3)\n",
    "        \n",
    "        \n",
    "        X = torch.tensor(X)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y       \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.X_train[idx], self.y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpwgeLcEf8qS"
   },
   "outputs": [],
   "source": [
    "path = 'ocean-eddy-detection/' # local\n",
    "trainDataset = TrainDataset(path)\n",
    "train_dataloader = DataLoader(trainDataset, batch_size=6, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator_data = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iterator_data)\n",
    "print(img.shape)\n",
    "plt.imshow(img[0][0], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Vertical velocity')\n",
    "plt.show()\n",
    "plt.imshow(label[0], origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Label')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "no9o8l4hf8qU"
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    Our modified Unet :\n",
    "    Use of padding to keep size of input in output easily.\n",
    "    Use of batchnorm2d after Conv2d\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downblock1 = nn.Sequential(\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(4, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downblock2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.downblock3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),          \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.middleU = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.upblock3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding='same', padding_mode='replicate'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 4, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.downblock1(x)\n",
    "\n",
    "        x2 = self.downblock2(x1)\n",
    "\n",
    "        x3 = self.downblock3(x2)\n",
    "\n",
    "        xmiddle = self.middleU(x3)\n",
    "\n",
    "        xup0_1 = torch.cat((x3,xmiddle), dim=1)\n",
    "        xup1 = self.upblock1(xup0_1)\n",
    "\n",
    "        xup1_2 = torch.cat((x2,xup1), dim=1)\n",
    "        xup2 = self.upblock2(xup1_2)\n",
    "\n",
    "        xup2_3 = torch.cat((x1,xup2), dim=1)\n",
    "        xup3 = self.upblock3(xup2_3)\n",
    "\n",
    "        return xup3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapted_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "      \n",
    "    def forward(self, prediction, target):\n",
    "      mask = target[0]!=3\n",
    "      y = target[:,mask]\n",
    "      ypred = prediction[:,:,mask]\n",
    "      return self.loss(ypred, y)\n",
    "\n",
    "#EXEMPLE\n",
    "#loss = Adapted_loss()\n",
    "#loss(ypred, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UKMFiIKf8qW"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move input and target tensors to the device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X).to(device)\n",
    "        pred = pred.to(dtype=torch.float64)\n",
    "    \n",
    "        loss = loss_fn(pred, y.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss and accuracy\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKPk7gtNf8qY",
    "outputId": "7544fcb4-977c-4a3c-fa0d-a000cd6de853"
   },
   "outputs": [],
   "source": [
    "model = Unet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "loss_fn = Adapted_loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZ_ic4Bdf8qY",
    "outputId": "f083d311-1958-4a29-94a9-731f8f10554d"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUX_AxO3f8qZ"
   },
   "outputs": [],
   "source": [
    "x_test, label_test = next(iter(train_dataloader))\n",
    "\n",
    "print(label_test.shape)\n",
    "x_test = x_test.to(device)[0]\n",
    "plt.imshow(label_test[0], origin='lower')\n",
    "plt.show()\n",
    "plt.imshow(np.argmax(model(x_test.reshape(1,4,357,717)).softmax(dim=1).cpu().detach().numpy(), axis=1)[0], origin='lower')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e567364bc8ddf153d80a9d3509d1b5709c952b74d6848487d8a2c235834a8128"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
